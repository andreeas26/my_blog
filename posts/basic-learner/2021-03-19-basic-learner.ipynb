{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "aliases:\n",
    "- /fastpages/jupyter/learner/machine learning/2021/03/19/basic-learner\n",
    "author: Andreea Sandu\n",
    "badges: true\n",
    "categories:\n",
    "- fastpages\n",
    "- jupyter\n",
    "- learner\n",
    "- machine learning\n",
    "date: '2021-03-19'\n",
    "description: How to write from scratch a SGD loop\n",
    "hide: false\n",
    "output-file: 2021-03-19-basic-learner.html\n",
    "title: A basic Learner\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro\n",
    "I have recently started the [fastai course v4](https://course.fast.ai/) after many postponements. Although I have been working with AI for a few years now I always thought Jeremy Howard's approach of teaching it's very interesting and ... different. Learning the _whole game_ from the start it's more appealing than learning what a tensor is. From the very start the course takes you through the whole process of developing an AI model and into deployment. I believe when the first version of the course was published AI was just beginning to be popular and many of the libraries and resources were _not yet invented_. Now they have all these cool tools to present the entire lifecycle while the students can better see the big picture.  This is just a testament of how much the community has evolved and how the _fastai_ team has succesfully managed to keep up the pace!\n",
    "\n",
    "fastai through the voice of Rachel Thomas encourages its students to start blogging. Another uncommon idea. What learning deep learning has anything to do with writing? [Here](https://medium.com/@racheltho/why-you-yes-you-should-blog-7d2544ac1045) you can find some good points about why blogging helps. Through this blog post (and maybe others that will follow) I want to test their hypothesis. I just want to try it and see how it goes.\n",
    "\n",
    "So, without further ado let's get our hands dirty with some deep learning. \n",
    "\n",
    "![](dl_meme.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write a basic Learner\n",
    "In this section I'll explain how I created a **_Learner_** from scratch by following the 7 steps for developing any deep learning model. \n",
    "\n",
    "\n",
    "\n",
    "1.   Initialize the parameters of the network;\n",
    "2.   Predict on some input data;\n",
    "3.   Compute the loss function. In other words how the predictions are different from the target (what we actually want);\n",
    "4.   Compute the parameters' gradients;\n",
    "5.   Update the parameters using backpropagation;\n",
    "6.   Repeat\n",
    "7.   Stop\n",
    "\n",
    "\n",
    "For this purpose I'll use the same MNIST_SAMPLE dataset that it's used in the book. It only contains the digits three and seven, so we'll start simple by resolving a binary classification problem. The fastai library provides some useful and easy to understand functions in order to get up and going realy quickly. I will not go into details about them. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "from fastai.vision.all import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#3) [Path('C:/Users/andreea.sandu/.fastai/data/mnist_sample/labels.csv'),Path('C:/Users/andreea.sandu/.fastai/data/mnist_sample/train'),Path('C:/Users/andreea.sandu/.fastai/data/mnist_sample/valid')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "path = untar_data(URLs.MNIST_SAMPLE)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train/3/7463.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train/3/21102.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train/3/31559.png</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                name  label\n",
       "0   train/3/7463.png      0\n",
       "1  train/3/21102.png      0\n",
       "2  train/3/31559.png      0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "labels_df = pd.read_csv(path/\"labels.csv\")\n",
    "labels_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('C:/Users/andreea.sandu/.fastai/data/mnist_sample/train/3'),Path('C:/Users/andreea.sandu/.fastai/data/mnist_sample/train/7')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "(path/\"train\").ls()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def load_img(img_path):\n",
    "  img = Image.open(path/img_path)\n",
    "  # transform the image into a vector and rescale it to [0, 1]\n",
    "  img = tensor(img).view(28*28).float()/255 \n",
    "  return img\n",
    "\n",
    "train_df = labels_df.loc[labels_df['name'].str.contains('train'), :]\n",
    "test_df = labels_df.loc[labels_df['name'].str.contains('valid'), :]\n",
    "\n",
    "train_dst = [(load_img(row['name']), tensor(row['label'])) for _, row in train_df.iterrows()]\n",
    "test_dst  = [(load_img(row['name']), tensor(row['label'])) for _, row in test_df.iterrows()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([784]), torch.Size([]), torch.Size([784]), torch.Size([]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "train_dst[0][0].shape, train_dst[0][1].shape, test_dst[0][0].shape, test_dst[0][1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is my implementation of a basic Learner that puts together all the necessary \"ingredients\" in order to train simple neural network:\n",
    "- `dls`: the data\n",
    "- `model`: the neural network\n",
    "- `opt_func`: optimization rule\n",
    "- `loss_func`: it's used by the model\n",
    "- `metrics`: the metrics we care about\n",
    "\n",
    "The function that does all the work is `fit()`. It updates the parameters of the network a number of `epochs` by geting a batch of input - target pairs from the train DataLoader (specified in `dls` parameter). The predictions from each batch are compared with the targets and based on these differences the gradiends are updated with a step set by `lr`. The last for loop it's for validation, where we are intereseted in the model performance on unseen data and its parameters are **not** updated. We're also do some brief logging after each epoch ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicLearner:\n",
    "  __name__ = 'BasicLearner'\n",
    "  __repr__ = basic_repr('dls,model,opt_func,loss_func,metrics')\n",
    "\n",
    "  def __init__(self, dls, model, opt_func, loss_func, metrics):\n",
    "    store_attr('dls,model,opt_func,loss_func,metrics')\n",
    "  \n",
    "  def _reset(self):\n",
    "    self.train_loss = []\n",
    "    self.val_loss = []\n",
    "\n",
    "  def fit(self, epochs=10, lr=1e-2):\n",
    "    opt = self.opt_func(self.model.parameters(), lr=lr)\n",
    "    self._reset()\n",
    "\n",
    "    for e in range(epochs):\n",
    "      batch_train_loss = []\n",
    "      for x,y in self.dls.train:\n",
    "        pred = self.model(x)\n",
    "        loss = self.loss_func(pred, y)\n",
    "        batch_train_loss.append(loss)\n",
    "        opt.zero_grad()\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "      batch_train_loss = tensor(batch_train_loss)\n",
    "\n",
    "      batch_val_loss = []\n",
    "      val_metrics = []\n",
    "      with torch.no_grad():\n",
    "        for x,y in self.dls.valid:\n",
    "          pred = self.model(x)\n",
    "          loss = self.loss_func(pred, y)\n",
    "          batch_val_loss.append(loss)\n",
    "          m = self.metrics(pred, y)\n",
    "          val_metrics.append(m)\n",
    "\n",
    "      batch_val_loss = tensor(batch_val_loss)\n",
    "      val_metrics = tensor(val_metrics)\n",
    "\n",
    "      self.train_loss.append(batch_train_loss.mean())\n",
    "      self.val_loss.append(batch_val_loss.mean())\n",
    "\n",
    "      msg = f\"Epoch {e}/{epochs}: \" \\\n",
    "        f\"Train loss {batch_train_loss.mean():.4f} \" \\\n",
    "        f\"Valid loss {batch_val_loss.mean():.4f} \" \\\n",
    "        f\"{self.metrics.__name__} {val_metrics.mean():.2f}\"\n",
    "      print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "bs = 128\n",
    "train_dl = DataLoader(train_dst, batch_size=bs)\n",
    "test_dl = DataLoader(test_dst, batch_size=bs, shuffle=False)\n",
    "dls = DataLoaders(train_dl, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "class Linear:\n",
    "  __name__ = \"Linear\"\n",
    "  __repr__ = basic_repr('size,w,b')\n",
    "\n",
    "  def __init__(self, size):\n",
    "    self.w, self.b = self._init_params(size), self._init_params(size[1])\n",
    "    store_attr('size')\n",
    "\n",
    "  def _init_params(self, size, std=1.0):  \n",
    "    return (torch.randn(size, dtype=torch.float32)*std).requires_grad_()\n",
    "\n",
    "  def __call__(self, x):\n",
    "    res = x@self.w + self.b\n",
    "\n",
    "    return res\n",
    "  \n",
    "  def get_parameters(self):\n",
    "    return [self.w, self.b]\n",
    "\n",
    "class Basic2LayersNet:\n",
    "  __name__ = \"Basic2LayersNet\"\n",
    "  __repr__ = basic_repr('l1_size,l2_size,sigmoid')\n",
    "\n",
    "  def __init__(self, l1_size, l2_size, sigmoid=True):\n",
    "    store_attr('l1_size,l2_size,sigmoid')\n",
    "\n",
    "    self.l1 = Linear(l1_size)\n",
    "    self.l2 = Linear(l2_size)\n",
    "\n",
    "    self.sigmoid = sigmoid\n",
    "\n",
    "  def __call__(self, xb):\n",
    "    res = self.l1(xb)\n",
    "    res = res.max(tensor(0.0)) #ReLU \n",
    "    res = self.l2(res)\n",
    "\n",
    "    if sigmoid:\n",
    "      return res.sigmoid()\n",
    "    return res\n",
    "  \n",
    "  def parameters(self):\n",
    "    params = []\n",
    "    params += self.l1.get_parameters()\n",
    "    params += self.l2.get_parameters()\n",
    "    for p in params:\n",
    "      yield p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "class BasicOptim:\n",
    "  __name__ = \"BasicOptim\"\n",
    "  __repr__ = basic_repr('parameters,lr')\n",
    "  def __init__(self, parameters, lr): self.parameters, self.lr = list(parameters), lr\n",
    "  \n",
    "  def step(self, *args, **kwargs):\n",
    "    for p in self.parameters: p.data -= p.grad * self.lr\n",
    "  \n",
    "  def zero_grad(self, *args, **kwargs):\n",
    "    for p in self.parameters: p.grad = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "def mnist_loss(predictions, targets):\n",
    "    return torch.where(targets==1, 1-predictions, predictions).mean()\n",
    "\n",
    "def mnist_accuracy(predictions, targets):\n",
    "    correct = (predictions > 0.5) == targets\n",
    "    return correct.float().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| include: false\n",
    "basic_net = Basic2LayersNet(l1_size=(28*28, 30), l2_size=(30, 1))\n",
    "basic_net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how it works on the MNIST_SAMPLE dataset. You can check the implemention of all the parameters given to `BasicLearner` class, on github.\n",
    "The basic network will train for 20 epochs with a learning rate of 0.1. After just a few epochs the model has over 90% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BasicLearner(dls=<fastai.data.core.DataLoaders object at 0x00000214F33F9D68>, model=Basic2LayersNet(l1_size=(784, 30), l2_size=(30, 1), sigmoid=True), opt_func=<class '__main__.BasicOptim'>, loss_func=<function mnist_loss at 0x00000214F334FB70>, metrics=<function mnist_accuracy at 0x00000214F7B0D0D0>)\n"
     ]
    }
   ],
   "source": [
    "learn = BasicLearner(dls, basic_net, BasicOptim, mnist_loss, mnist_accuracy)\n",
    "print(learn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/20: Train loss 0.2138 Valid loss 0.3241 mnist_accuracy 0.67\n",
      "Epoch 1/20: Train loss 0.1307 Valid loss 0.1945 mnist_accuracy 0.81\n",
      "Epoch 2/20: Train loss 0.0980 Valid loss 0.1210 mnist_accuracy 0.88\n",
      "Epoch 3/20: Train loss 0.0814 Valid loss 0.0959 mnist_accuracy 0.90\n",
      "Epoch 4/20: Train loss 0.0663 Valid loss 0.0856 mnist_accuracy 0.91\n",
      "Epoch 5/20: Train loss 0.0612 Valid loss 0.0749 mnist_accuracy 0.93\n",
      "Epoch 6/20: Train loss 0.0572 Valid loss 0.0707 mnist_accuracy 0.93\n",
      "Epoch 7/20: Train loss 0.0519 Valid loss 0.0668 mnist_accuracy 0.93\n",
      "Epoch 8/20: Train loss 0.0490 Valid loss 0.0639 mnist_accuracy 0.94\n",
      "Epoch 9/20: Train loss 0.0463 Valid loss 0.0603 mnist_accuracy 0.94\n",
      "Epoch 10/20: Train loss 0.0440 Valid loss 0.0567 mnist_accuracy 0.94\n",
      "Epoch 11/20: Train loss 0.0424 Valid loss 0.0539 mnist_accuracy 0.95\n",
      "Epoch 12/20: Train loss 0.0408 Valid loss 0.0524 mnist_accuracy 0.95\n",
      "Epoch 13/20: Train loss 0.0393 Valid loss 0.0511 mnist_accuracy 0.95\n",
      "Epoch 14/20: Train loss 0.0383 Valid loss 0.0493 mnist_accuracy 0.95\n",
      "Epoch 15/20: Train loss 0.0375 Valid loss 0.0475 mnist_accuracy 0.95\n",
      "Epoch 16/20: Train loss 0.0365 Valid loss 0.0458 mnist_accuracy 0.95\n",
      "Epoch 17/20: Train loss 0.0355 Valid loss 0.0445 mnist_accuracy 0.96\n",
      "Epoch 18/20: Train loss 0.0345 Valid loss 0.0437 mnist_accuracy 0.96\n",
      "Epoch 19/20: Train loss 0.0336 Valid loss 0.0430 mnist_accuracy 0.96\n"
     ]
    }
   ],
   "source": [
    "learn.fit(20, lr=0.1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.12 64-bit ('fastai': conda)",
   "metadata": {
    "interpreter": {
     "hash": "c933194f756867ba8dd5da9b0b34dc0031cd583cf07f9dcb657353c4524c172d"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12-final"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
